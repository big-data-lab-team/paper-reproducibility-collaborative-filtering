\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=blue}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{subcaption}
\usepackage{nicefrac}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\newcommand{\comment}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{blue}\textbf{Comment}:
      #1}}}

\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}

\begin{document}

\title{Predicting computational reproducibility of scientific pipelines using collaborative filtering}

\author{Soudabeh Barghi, Lalet Scaria, Tristan Glatard\\
  Department of Computer Science and Software Engineering\\ Concordia University, Montreal, Quebec, Canada\\
  {first.last}@concordia.ca\\
  $^*$ These authors have contributed equally
}

\maketitle

\begin{abstract}
\end{abstract}


\section{Introduction}

Computational reproducibility, the property through which
computational results can be recomputed over time and
space~\cite{stodden}, has become a critical component of scientific
methodology with the rise of the reproducibility crisis in several
domains~\cite{xxx}. Among the factors hampering computational
reproducibility, infrastructural characteristics such as the operating
system play an important role. In neurosciences, our primary field of
interest, several studies have shown the effect of the operating
system on computational results. However, conducting such
reproducibility studies at scale is cumbersome due to the execution
time of data analysis pipelines, which easily exceeds a few hours.

In this paper we investigate approximate methods to predict the
reproducibility of a computational analysis from the first files that
it produces. Our main intuition is that reproducibility errors are
caused by a reduced number of factors that originate in the analysis
pipeline and input data. 

% Computational reproducibility is an issue, for instance among
% different operating systems (refer to Glatard FINF 2015,
% Gronenschild 2012).

% Scientific data analysis pipeline executions are long (give examples
% from neuroimaging).

% Refer to Germain et al 2008.

% Define subjects, pipelines.

% Problem: predict the reproducibility of pipeline files from other
% subjects and the first files produced by a pipeline. Restrict the
% study to binary classification.




\section{Method}

what is specific to our problem compared to regular collaborative filtering:
%+ subjects are equivalent to users. Not all subjects have the same input data. Anatomical variability, acquisition variability (e.g., 1 subject may have multiple T1s).
%+ files are produced in a specific order (movies aren't),
% which add constraints on the training set (cannot sample late files and not early ones).
%+ utility matrix is not sparse: we can potentially populate it completely. Which means that we can decide precisely which samples we need (active sampling). Therefore we can take the first row and first column to avoid cold start issues.

\subsection{Collaborative filtering using ALS}

We consider the collaborative filtering method as implemented in Spark's MLlb.
% Summarize Koren, Bell and Volinsky: https://dl.acm.org/citation.cfm?id=1608614

% Explain that you have binary classes (rounding)

% Biases


\subsection{Sampling the training set}

We investigated the following sampling techniques for the training set. 
In each method, we included the first row of the matrix (first 
generated file of each subject) and a random column (all files of a 
random subject) to avoid cold start issues. 

% 0. random unreal (baseline)
\paragraph{Random Unreal}
The training set is sampled randomly with no restriction, that is,
regardless of the file generation time. This method will be used as a
baseline for comparison with other methods, although it is not
realistic. Figure~\ref{fig:Random-Unreal-Sample-Training-set} represents 
a training set sampled using this method.


% 2. columns
\paragraph{Complete Columns}
The training set is sampled by randomly selecting complete columns in
the matrix, that is, complete subject executions. The last selected
column might be incomplete to meet the exact training ratio. This 
method is realistic: it corresponds to a situation where the 
collaborative filtering method will predict the reproducibility in the 
remaining subjects from the subjects in the training set. 
Figure~\ref{fig:Columns-Sample-Training-set} represents a training set 
sampled using this method.


% 3. rows
\paragraph{Complete Rows} The training set is sampled by selecting complete 
rows in the matrix, that is, the first files produced by
every execution. The last selected row might be incomplete to meet the
exact training ratio. This method is realistic: it corresponds to a 
situation where the processing of all the subjects is launched and 
interrupted before the execution is complete. The collaborative 
filtering method will then predict the reproducibility of the remaining 
files. Figure~\ref{fig:Rows-Sample-Training-set} represents a training 
set sampled using this method.
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\columnwidth]{figures/5vs7_random-unreal_04_training}
  		\caption{Random Unreal method}
  		\label{fig:Random-Unreal-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
  		\includegraphics[width=\columnwidth]{figures/5vs7_columns_04_training}
  		\caption{Complete Columns method}
  		\label{fig:Columns-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
  		\includegraphics[width=\columnwidth]{figures/5vs7_rows_04_training}
  		\caption{Complete Rows method}
  		\label{fig:Rows-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
 		\includegraphics[width=\columnwidth]{figures/5vs7_random-real_04_training}
		\caption{Random Subjects method}
  		\label{fig:Uniform-S-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
  		\includegraphics[width=\columnwidth]{figures/5vs7_diagonal_04_training}
  		\caption{(Uniform) method }
  		\label{fig:Diagonal-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
  		\includegraphics[width=\columnwidth]{figures/5vs7_random-triangular-largest_04_training}
  		\caption{(Triangular-L) method}
  		\label{fig:triangular-L-Sample-Training-set}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
  		\includegraphics[width=\columnwidth]{figures/5vs7_random-triangular-smallest_04_training}
  		\caption{(Triangular-S) method}
  		\label{fig:triangular-S-Sample-Training-set}
	\end{subfigure}
	\caption{Sample training set for different methods with a training ratio of 0.4.}		
\end{figure}

\paragraph{Random Subjects -- RS} This method builds the 
training set by selecting the files from random subjects 
until the training ratio is reached. The file selected in a 
subject is the file with the lowest index in this subject that has not 
been already selected in the training set. This method is realistic as 
files are sampled according to their production timestamps. 
Figure~\ref{fig:Uniform-S-Sample-Training-set} represents a training 
set sampled using this method.

\paragraph{Random File Numbers (Uniform) -- RFNU}

The number of files selected for every subject is randomly selected in
a uniform distribution $U(\textit{a},\textit{b})$, where \textit{\textbf{b}} is set to the total
number of files $N_{f}$ and \textit{\textbf{a}} is set according to training ratio ($\alpha$) as follow:

\[
  \begin{cases}
          \textit{\textbf{a}} = 0      & \text{if $\alpha \leq 0.5$ }\\
          
          \textit{\textbf{a}} = (2\alpha - 1) N_{f} & \text{if $\alpha > 0.5$}
  \end{cases}
\]


This method is realistic as files are sampled according to their production timestamps.
Due to sampling issues, it is possible that the actual training ratio obtained with this method
does not match the target one. We check that the difference between the target and real training ratios
was lower than 0.01.\\ 

\paragraph{Random File Numbers (Triangular) -- RFNT}
The number of files selected for every subject is randomly selected in
a triangular distribution with parameters \textit{\textbf{a}}, \textit{\textbf{c}} 
and \textit{\textbf{b}} where \textit{\textbf{b}} is set to $N_{f}$ and the other 
two parameters could be set according to two different approaches. By considering $\alpha$ 
as training ratio, the distribution average equals to $\alpha N_{f}$ which that means 
for $\alpha$ less than $\nicefrac{1}{3}$ the lower limmit \textit{\textbf{a}} would 
get a negative value in this regard, therefore to avoid this issue the uniform distribution 
is applied for $\alpha < \nicefrac{1}{3}$. But for $\alpha \leq \nicefrac{1}{3}$ the following 
approaches are applicable. This distribution aims at sampling files produced towards 
the end of the pipeline execution.

Two approches are tried to apply this method:
\begin{enumerate}
\item Largest-a (RFNT-L):  
This approach of triangular distribution with parameters \textit{\textbf{a}}, \textit{\textbf{c}} 
and \textit{\textbf{b}} where \textit{\textbf{b}} is set to $N_{f}$ and the two other 
parameters are reperesent the limmited bond and mode of the distribution respectively. 
The \textit{\textbf{a}} equals to $\nicefrac {N_{f}(3\alpha-1)}{2}$ while \textit{\textbf{c}} 
is considered as equal as \textit{\textbf{a}}, $(\textit{\textbf{c}} = \textit{\textbf{a}})$.\\

In this approach there is a concern to eliminate the possibility of having less 
than the first two generated files of any subjects in the sampled training set. 
The possibility of missing some subjects in training set can happen to some of them due to 
random selection of subjects or even meeting the determined training ratio before 
having the chance of being selected.

\item Smallest-a (RFNT-S):
This approach of triangular distribution with parameters \textit{\textbf{a}}, \textit{\textbf{c}} 
and \textit{\textbf{b}} where \textit{\textbf{b}} is set to $N_{f}$ and the
two other parameters are reperesent the limmited bond and mode of the distribution respectively. 
The \textit{\textbf{a}} is set to $0$  while \textit{\textbf{c}} equals to $N_{f}(3\alpha-1)$.\\
In this approach of utilizing triangular distribution,despite of Largest-a approch, there is a possibility 
that some subjects be reperesented in the training set just by their first generated file and no more files.

\end{enumerate}

\section{Dataset}

We collected data to evaluate the computational reproducibility of analysis
pipelines of the Human Connectome Project~\cite{general-hcp}. We
processed a set S of 94 subjects randomly selected in the S500 HCP
release~\todo{URL} in three execution conditions with different
versions of the CentOS operating system (5.?, 6.? and 7.?), using the
PreFreesurfer, Freesurfer, PostFreesurfer and fMRIVolume pipelines
described in~\cite{hcp-pipelines} and available at \todo{URL}. For
each pipeline, we identified the set F of files produced for all
subjects in all conditions. For each condition pair and each pipeline,
we computed a binary difference matrix D of size $|F|\times|S|$, where $D_{i,j}$ is true
if file $i$ of subject $j$ was different in each condition. Rows of
$M$ are ordered by ascending file modification time in the pipeline.

Figure~\ref{fig:utility metrices} shows the difference matrices
obtained for the PreFreesurfer pipeline. The computational
reproducibility of the PreFreesurfer pipeline varies across subjects,
which motivates our study. Nevertheless, some files behave
consistently across all subjects, leading to complete black or white
lines. The ratio of positive elements in utility matrix of CentOS5 vs CentOS6(C5C6),  CentOS5 vs CentOS7(C5C7) and  CentOS6 vs CentOS7(C6C7) are
$0.34$, $0.79$ and $0.79$ respectively.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
	\includegraphics[width=\columnwidth]{figures/utility_5vs6_PFS}
  \caption{Utility matrix of CentOS5 vs CentOS6}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
	 \includegraphics[width=\columnwidth]{figures/utility_5vs7_PFS}
  \caption{Utility matrix of CentOS5 vs CentOS7}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
	\includegraphics[width=\columnwidth]{figures/utility_6vs7_PFS}
  \caption{Utility matrix of CentOS6 vs CentOS7}
  \label{fig:utility metrices}
  \end{subfigure}
\end{figure}

% Describe your dataset: pipeline(s) used, input data, operating systems (CentOS5, 6, 7), matrix.

% 1. Prefreesurfer (what you have)
% 2. Freesurfer, with the same subjects as in Prefreesurfer.
% 3. PostFreesurfer
% 4. fmriVolume

\section{Results}


Four experiments have been conducted to evaluate the performance of our 
predictions using (1) ALS without biases, (2) ALS with biases, (3) Biases only. 
Results will be evaluated on the complete datasets and specifically on the files that
do not behave consistently across subjects (\todo{explain this in introduction}).
\todo{the last sentence just applies for Traiangular-L and on the case that I called 
pure accuracy, other results of ALS, ALS-biases are not applied in this case}
We evaluate prediction methods using accuracy, sensitivity and specificity defined as follows:
\[
        Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\]
\[
        Sensitivity = \frac{TP}{TP+FN}
\]
\[
        Specificity = \frac{TN}{TN+FP}
\]
Where TP is the number of True Positives (correctly predicted errors), 
FP is the number of False Positives, TN is the number of true negatives 
and FN is the number of false negatives.

We compare our methods to two references, (1) a dummy classifier that always predicts the dominant class.
as the dominent value of the utility matrix (1 or 0), (2) the Random Unreal method, used as the baseline sampling technique.

\subsection{ALS without biases}
In this phase of the experiment, the Alternating Least Squares 
technique of Collaborative Filtering is applied for all provided 
methods. Our four sampling techniques are compared in 
Figure~\ref{fig:ALS-Failure} and the results are described in the next 
sub-sections.

\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/AlS-Failure-5vs6}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/AlS-Failure-5vs7}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/AlS-Failure-6vs7}
        \end{subfigure}
        \caption{Average accuracy of Complete Columns, RS, Complete Rows and Random Unreal methods in three different condition pairs.}
        \label{fig:ALS-Failure}
\end{figure}

\subsubsection{Random Unreal}
This method has a great performance with absolute accuracy of more than $0.95$ 
even by accessing to small training size of 10 percent. As it mentioned earlier 
this method is not applicable in pipline due to timestamp consideration but it 
is a good criterion to estimate the performance of other methods and try to modify 
the proposed methods in order to their accuracy be as closed as possible to the 
Random Unreal method ones.  


\subsubsection{Complete Columns}
The Complete Columns method has almost a constant accuracy even if the training ratio increased and it
is almost two times less than dummy classifier in more differentiated
condition pairs (CentOS5 vs CentOS7 and CentOS6 vs CentOS7). But in the
best case which it means in similar conditions the Complete Columns
accuracy is as much accurate as the Dummy classifier is, $0.65$
percent.
\\
\subsubsection{Complete Rows}
This method has some flactuation over the gowth of training set size. These falactuations are more 
frequent in condition pairs which have higher difference positive ratio. 
Also by enlarging the training ratio specially between $0.3$ to $0.7$, 
we can see dummy classifier has better results than Complete Rows ones. 
In other words despite of better performance of Complete Rows than 
dummy classifier at the last 10 percent of the graph, its accuracies 
are less than dummy classifiers ones.  
\\
\subsubsection{Random Subjects-RS}
Same to the Complete Row method, RS method has some flactuation but not as frequent 
as Complete Rows method. Althogh there are some up and down trends specialy with 
training ratio range of $0.2$ to $0.6$ but its constant growth trend after 
training ratio $0.3$ in condition pairs with high difference positive ratio is considerable.
The method can be $0.99$ percent accurate when it has over 70 percent of the data however 
this accuracy can be achived with higher training ratio (over 80 percent) for C5C6.  
\\
\\
\noindent Figure~\ref{fig:ALS-Succeed} represents
the accuracy graph of all Random kind methods with comparision to dummy
classifier over the increasement of the training ratio.\\
\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/ALS-Succeed-5vs6}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/ALS-Succeed-5vs7}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS/ALS-Succeed-6vs7}
        \end{subfigure}
        \caption{Average performance of RFNU methods in three different condition pairs}
        \label{fig:ALS-Succeed}
\end{figure}
\subsubsection{Random File Numbers(Uniform) - RFNU}
The continuous rise of RFNU accuracy trend can be observed in Figure~\ref{fig:ALS-Succeed}.
However a 0.03 drop is recorded in C6C7 between training ratio size of $0.8$ to $0.9$. 
This method takes over the dummy accuracy after having a training size more than 40 percent 
in condition pairs of C5vsC7 and C6vsC7 but it always shows better acuracy than 
dummy classifier ($0.73$ percent with traininfg ratio of $0.1$).\newline

% ALS-RFNT approaches
\indent As it mentioned earlier, for $\alpha < \nicefrac{1}{3}$ RFNT approaches are not applicable, 
therefor for training ratio less than $0.4$ RFNU method is employed. As a result, in terms of 
evaluating the performance of RFNT-L and RFNT-S approches, only those accuracy results are 
considered which their training set are equal or geather than 0.4 percent of the whole utility matrix.\newline 

\subsubsection{RFNT-L}

In Figure~\ref{fig:ALS-Succeed}, the results show that RFNT-L performance is always more accurate than
dummy classifier even in its lowest accuracy value observed at training ratio of $0.6$ in C5C6 ($0.78$ percent).
Dispite of this sudden, RFNT-L method has slight growing trend throughout the 
increment of training ratio and its accuracy is always more than $90\%$. 

\subsubsection{RFNT-S}
Figure~\ref{fig:ALS-Succeed} shows the overall performance of RFNT-S methods in three different condition pairs.
Similar to RFNT-L, this method also has constant accuracy increament with maximum less accuracy of 0.07 than 
Random Unreal method throughout the experiment of C5C7 and C6C7.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adding biases}
In this part of the experiment, the effect of both subjects' and files' biases are considered in the predicted value. 
Our three random sampeling techniques are compered in Figure~\ref{fig:ALS-Bias-PFS} accross the accretion of training 
ratio;$\alpha$; and the results are reported at the following subsections.

\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/ALS-Bias-5vs6-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/ALS-Bias-5vs7-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/ALS-Bias-6vs7-PFS}
        \end{subfigure}
        \caption{Average performance of random methods in three different condition pairs with cosidering Bias of both subjects and files}
        \label{fig:ALS-Bias-PFS}
\end{figure}


\subsubsection{RFNU}
The obtained results show that by sampling at least half of the utility matrix, 
the predicted accuracy would be as accurate as the baseline (Random Unreal technique) 
but for the $\alpha$ of less than $50\%$ this is not true. Actually there is a 
difference range of $0.2$ to $0.7$ between the accuracy of Random Unreal and RFNU techniques for
$\alpha <0.5$ in C5C7 and C6C7 but this range chanhe to $(0.07,0.17)$ in C5C6. 

\subsubsection{RFNT-L}
As it mentioned earlier RFNT-L technique is applicable for $\alpha \leq \nicefrac{1}{3}$, therefore its 
accuracy description is summerized for $\alpha > 0.3$. This accuracy of this technique is always higher than 
dummy classifier. Results show that this RFNT-L reaches to $100\%$ accuracy by $\alpha >0.8$ in condition pairs
with more errors, however the baseline never reached this high accuracy throughout of whole experiment.

\subsubsection{RFNT-S}
This sampling technique is as precise as Random Unreal with maximum difference gap of 0.07 between them which that 
happend in C5C7 at $\alpha = 0.7$ with accuracy of $0.91$. 














% General conclution on results of ALS without Bias 
In general the accuracy pattern of the two Complete Rows and 
RS methods show similar decline and grow behavior over the 
training ratio axis however it can be subject to considerable 
fluctuations in differentiated condition pairs. Dispite of their 
likely irrational behaviour, they always have better accuracy 
for the training size with less than $0.3$ in comparison to other 
methods. This better performance (around 75 percent)is almost three 
times higher than other methods in condition pairs with more 
differences but just 20 percent better when the conditions are more 
similar to each other (CentOS5 vs CentOS6) with accuracy of $90\%$.
Figure~\ref{fig:RS_Rows} represents
the accuracy graph of these two methods with comparision to dummy
classifier over the increasement of the training ratio.
\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.4\linewidth}
                \includegraphics[width=\columnwidth]{figures/RS_Rows_5vs6}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\linewidth}
                \includegraphics[width=\columnwidth]{figures/RS_Rows_5vs7}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\linewidth}
                \includegraphics[width=\columnwidth]{figures/RS_Rows_6vs7}
        \end{subfigure}
        \caption{Average performance of RS and Complete Rows methods in three different condition pairs}
        \label{fig:RS_RowS}
\end{figure}

All the other random methods, RS and both RFNT approaches 
are performing a continues growth Figure~\ref{fig:simple-methods} represents
the accuracy graph of all random methods with comparision to dummy
classifier over the increasement of the training ratio.. Although some considerable variation 
for both triangular approches have been observed (CentOS5 vs CentOS6) 
their constant rise leads us to the next phase of experiments which was 
adding up bias into the current ALS technique. 
The training set suffers from lacking the files which are almost generated towareds the end of pipeline.

\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.4\linewidth}
                \includegraphics[width=\columnwidth]{figures/simple-methods-5vs6}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\columnwidth]{figures/simple-methods-5vs7}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\linewidth}
                \includegraphics[width=\columnwidth]{figures/simple-methods-6vs7}
        \end{subfigure}
	\caption{Average performance of all methods in three different condition pairs}
	\label{fig:simple-methods}
\end{figure}
%-----------------------------------------------------------
\subsection{ALS with biases}

\subsubsection{RandomFile Numbers(Uniform) - RFNU}
Figure~\ref{fig:RFNU method} represents
the accuracy graph of RFNU method with comparision to dummy
classifier over the increasement of the training ratio.
\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNU-ALS-Bias-5vs6-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNU-ALS-Bias-5vs7-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNU-ALS-Bias-6vs7-PFS}
        \end{subfigure}
        \caption{Average performance of RFNU methods with considering Bias in three different condition pairs}
        \label{fig:RFNU method}
\end{figure}
\subsubsection{RNFT-L}
Figure~\ref{fig:RFNT-L method} represents
the accuracy graph of RFNT-L method with comparision to dummy
classifier over the increasement of the training ratio.
\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-L-ALS-Bias-5vs6-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-L-ALS-Bias-5vs7-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-L-ALS-Bias-6vs7-PFS}
        \end{subfigure}
        \caption{Average performance of RFNT-L methods  with considering Bias in three different condition pairs}
        \label{fig:RFNT-L method}
\end{figure}
\subsubsection{RFNT-S}
Figure~\ref{fig:RFNT-S method} represents
the accuracy graph of RFNTS method with comparision to dummy
classifier over the increasement of the training ratio.
\begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-S-ALS-Bias-5vs6-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-S-ALS-Bias-5vs7-PFS}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\linewidth}
                \includegraphics[width=\columnwidth]{figures/ALS-Bias/RFNT-S-ALS-Bias-6vs7-PFS}
        \end{subfigure}
        \caption{Average performance of RFNT-S methods  with considering Bias in three different condition pairs}
        \label{fig:RFNT-S method}
\end{figure}
%-----------------------------------------------------------
% Present your results: accuracy, ROC curves, transparency matrix, factors. 

% Try with different numbers of factors. 

\section{Discussion}

% Not all subjects behave the same, which motivates the Big Data approach. 

% Which sampling method is best

% Interpreting the factors? Factors reflect the pipeline definition. 

% How can this be used in practice

% What are the limitations

\section{Conclusion}

% Summary of the results and discussion. The take-home message.
%%%%%% why there is a drop in ALS-RFNT-L with 60% ratio?  
\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}


\end{document}
